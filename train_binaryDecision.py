# -*- coding: utf-8 -*-
"""
å¤šæ¨¡å‹ã€å¤šç­–ç•¥å¾®è°ƒå®éªŒæ¡†æ¶

åŠŸèƒ½ï¼š
åœ¨ä¸€ä¸ªäºŒåˆ†ç±»ä»»åŠ¡ä¸Šï¼Œè¯„ä¼°ä¸åŒ Transformer é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ BERT, RoBERTaï¼‰
å’Œä¸åŒå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼ˆå¦‚ LoRA, Prefix Tuning, BitFitï¼‰çš„æ•ˆæœã€‚

æ ¸å¿ƒä»»åŠ¡ï¼š
1.  ä»ç”¨æˆ·æä¾›çš„ JSON æ•°æ®é›†åŠ è½½æ•°æ®ã€‚
2.  å¯¹å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼ˆBackbonesï¼‰å’Œå¤šç§å¾®è°ƒç­–ç•¥ï¼ˆFine-tuning Methodsï¼‰è¿›è¡Œç»„åˆã€‚
3.  å¯¹æ¯ç§ç»„åˆè¿›è¡Œè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•ã€‚
4.  å°†è®­ç»ƒæ—¥å¿—ã€è¯„ä¼°æŒ‡æ ‡è®°å½•åˆ°æœ¬åœ°è¿è¡Œçš„ WandB æœåŠ¡ã€‚
5.  ä¿å­˜å¾®è°ƒåçš„æœ€ä½³æ¨¡å‹ã€‚
6.  è‡ªåŠ¨è·³è¿‡å·²ç»å®Œæˆçš„å®éªŒç»„åˆã€‚

è¿è¡Œç¤ºä¾‹ï¼š
python this_script_name.py --data_path ./your_data.json --output_dir ./experiment_results --num_epochs 3

JSON æ•°æ®æ ¼å¼è¦æ±‚ï¼š
[
  {"instruction": "some text here", "label": 1},
  {"instruction": "another text here", "label": 0}
]
"""
import os
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List

import torch
import numpy as np
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    EvalPrediction,
    set_seed,
)
from peft import (
    get_peft_model,
    LoraConfig,
    PrefixTuningConfig,
    TaskType,
    PeftModel,
    PeftConfig
)
import wandb

# --- å…¨å±€é…ç½® ---
# è®¾ç½®æ—¥å¿—è®°å½•
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
# ä¸ºä¿è¯å®éªŒå¯å¤ç°æ€§ï¼Œè®¾ç½®éšæœºç§å­
set_seed(42)

# å®šä¹‰æ”¯æŒçš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆBackbonesï¼‰
SUPPORTED_BACKBONES = [
    "bert-base-uncased",
    "roberta-base",
    "sentence-transformers/all-MiniLM-L6-v2",
]
# å®šä¹‰æ”¯æŒçš„å¾®è°ƒç­–ç•¥
SUPPORTED_FINETUNE_METHODS = [
    "full_finetune",  # å…¨é‡å¾®è°ƒ
    "freeze_base",    # å†»ç»“åŸºåº§ï¼Œåªè®­ç»ƒåˆ†ç±»å¤´
    "lora",           # Low-Rank Adaptation
    "prefix_tuning",  # Prefix Tuning
    "bitfit",         # åªè®­ç»ƒ bias å‚æ•°
]
# Weights & Biases é¡¹ç›®åç§°
WANDB_PROJECT_NAME = "vla-preference-classification-local"


# --- ä»»åŠ¡ 1: æ•°æ®åŠ è½½ä¸åˆ’åˆ† ---
def load_and_split_data(data_path: str, test_size: float = 0.1, val_size: float = 0.1) -> DatasetDict:
    """
    ä» JSON æ–‡ä»¶åŠ è½½æ•°æ®ï¼Œå¹¶åˆ’åˆ†ä¸ºè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†ã€‚

    Args:
        data_path (str): JSON æ•°æ®æ–‡ä»¶è·¯å¾„ã€‚
        test_size (float): æµ‹è¯•é›†æ‰€å æ¯”ä¾‹ã€‚
        val_size (float): éªŒè¯é›†æ‰€å æ¯”ä¾‹ï¼ˆåœ¨å‰©ä½™æ•°æ®ä¸­ï¼‰ã€‚

    Returns:
        DatasetDict: åŒ…å« 'train', 'validation', 'test' çš„æ•°æ®é›†å­—å…¸ã€‚
    """
    logging.info(f"â‘  ä» {data_path} åŠ è½½æ•°æ®...")
    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        logging.error(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
        raise

    # ç¡®ä¿æ•°æ®æ˜¯åˆ—è¡¨å½¢å¼
    if not isinstance(data, list):
        raise ValueError("JSON æ•°æ®å¿…é¡»æ˜¯ä¸€ä¸ªåˆ—è¡¨ (list of objects)ã€‚")

    texts = [item['instruction'] for item in data]
    labels = [item['label'] for item in data]
    
    # ç¬¬ä¸€æ¬¡åˆ’åˆ†ï¼šåˆ†å‡ºè®­ç»ƒ+éªŒè¯é›† å’Œ æµ‹è¯•é›†
    # ä½¿ç”¨ stratify ä¿è¯æ ‡ç­¾åˆ†å¸ƒåœ¨åˆ’åˆ†åä¿æŒä¸€è‡´
    train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(
        texts, labels, test_size=test_size, random_state=42, stratify=labels
    )
    
    # ç¬¬äºŒæ¬¡åˆ’åˆ†ï¼šä»è®­ç»ƒ+éªŒè¯é›†ä¸­åˆ†å‡ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
    # è°ƒæ•´éªŒè¯é›†æ¯”ä¾‹
    relative_val_size = val_size / (1 - test_size)
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        train_val_texts, train_val_labels, test_size=relative_val_size, random_state=42, stratify=train_val_labels
    )

    logging.info(f"æ•°æ®åˆ’åˆ†å®Œæ¯•ï¼š")
    logging.info(f"  - è®­ç»ƒé›†: {len(train_texts)} æ¡")
    logging.info(f"  - éªŒè¯é›†: {len(val_texts)} æ¡")
    logging.info(f"  - æµ‹è¯•é›†: {len(test_texts)} æ¡")

    # è½¬æ¢ä¸º HuggingFace çš„ Dataset æ ¼å¼
    train_dataset = Dataset.from_dict({"text": train_texts, "label": train_labels})
    val_dataset = Dataset.from_dict({"text": val_texts, "label": val_labels})
    test_dataset = Dataset.from_dict({"text": test_texts, "label": test_labels})

    return DatasetDict({
        "train": train_dataset,
        "validation": val_dataset,
        "test": test_dataset
    })


# --- ä»»åŠ¡ 2 & 3: æ¨¡å‹åŠ è½½ä¸å¾®è°ƒç­–ç•¥åº”ç”¨ ---
def get_model_and_tokenizer(backbone: str, finetune_method: str):
    """
    æ ¹æ®æŒ‡å®šçš„ backbone å’Œå¾®è°ƒç­–ç•¥åŠ è½½æ¨¡å‹å’Œ tokenizerã€‚

    Args:
        backbone (str): é¢„è®­ç»ƒæ¨¡å‹çš„åç§°æˆ–æœ¬åœ°è·¯å¾„ã€‚
        finetune_method (str): å¾®è°ƒç­–ç•¥çš„åç§°ã€‚

    Returns:
        Tuple[PreTrainedModel, PreTrainedTokenizer]: åŠ è½½å¥½çš„æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚
    """
    logging.info(f"â‘¡ åŠ è½½æ¨¡å‹: {backbone}ï¼Œä½¿ç”¨å¾®è°ƒç­–ç•¥: {finetune_method}")
    
    # åŠ è½½ Tokenizer
    try:
        tokenizer = AutoTokenizer.from_pretrained(backbone)
    except Exception as e:
        logging.error(f"åŠ è½½ Tokenizer '{backbone}' å¤±è´¥: {e}")
        raise

    # åŠ è½½åŸºç¡€æ¨¡å‹ï¼ŒæŒ‡å®šä¸ºäºŒåˆ†ç±»ä»»åŠ¡
    try:
        model = AutoModelForSequenceClassification.from_pretrained(backbone, num_labels=2)
    except Exception as e:
        logging.error(f"åŠ è½½æ¨¡å‹ '{backbone}' å¤±è´¥: {e}")
        raise

    # æ ¹æ®å¾®è°ƒç­–ç•¥è°ƒæ•´æ¨¡å‹å‚æ•°çš„å¯è®­ç»ƒæ€§
    if finetune_method == "full_finetune":
        # æ‰€æœ‰å‚æ•°éƒ½å¯è®­ç»ƒï¼Œæ— éœ€é¢å¤–æ“ä½œ
        pass
    elif finetune_method == "freeze_base":
        # å†»ç»“åŸºåº§æ¨¡å‹çš„æ‰€æœ‰å‚æ•°
        for param in model.base_model.parameters():
            param.requires_grad = False
    elif finetune_method == "bitfit":
        # åªè®­ç»ƒ bias å‚æ•°
        for name, param in model.named_parameters():
            if '.bias' not in name:
                param.requires_grad = False
    elif finetune_method == "lora":
        # åº”ç”¨ LoRA
        peft_config = LoraConfig(
            task_type=TaskType.SEQ_CLS,
            inference_mode=False,
            r=8,
            lora_alpha=16,
            lora_dropout=0.1
        )
        model = get_peft_model(model, peft_config)
    elif finetune_method == "prefix_tuning":
        # åº”ç”¨ Prefix Tuning
        peft_config = PrefixTuningConfig(
            task_type=TaskType.SEQ_CLS,
            num_virtual_tokens=20
        )
        model = get_peft_model(model, peft_config)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„å¾®è°ƒæ–¹æ³•: {finetune_method}")

    # æ‰“å°å¯è®­ç»ƒå‚æ•°çš„æ•°é‡
    if isinstance(model, PeftModel):
        model.print_trainable_parameters()
    else:
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        logging.info(
            f"å¯è®­ç»ƒå‚æ•°: {trainable_params} | æ€»å‚æ•°: {total_params} | å¯è®­ç»ƒæ¯”ä¾‹: {100 * trainable_params / total_params:.2f}%"
        )
        
    return model, tokenizer


# --- ä»»åŠ¡ 4: æ•°æ®é¢„å¤„ç† ---
def create_preprocess_function(tokenizer):
    """
    åˆ›å»ºç”¨äºæ•°æ®é›†çš„é¢„å¤„ç†å‡½æ•°ã€‚

    Args:
        tokenizer: HuggingFace Tokenizer å®ä¾‹ã€‚

    Returns:
        Callable: ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå¯¹æ•°æ®é›†è¿›è¡Œç¼–ç ã€‚
    """
    def preprocess(examples):
        # å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œè¿›è¡Œå¡«å……å’Œæˆªæ–­
        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)
    return preprocess


# --- ä»»åŠ¡ 5 & 6 & 7: è®­ç»ƒã€è¯„ä¼°ä¸ä¿å­˜ ---
def compute_metrics(p: EvalPrediction) -> Dict[str, float]:
    """
    è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆè¿™é‡Œæ˜¯å‡†ç¡®ç‡ï¼‰ã€‚

    Args:
        p (EvalPrediction): Trainer çš„è¯„ä¼°é¢„æµ‹ç»“æœã€‚

    Returns:
        Dict[str, float]: åŒ…å«å‡†ç¡®ç‡çš„å­—å…¸ã€‚
    """
    preds = np.argmax(p.predictions, axis=1)
    return {"accuracy": (preds == p.label_ids).mean()}

def run_training_pipeline(
    backbone: str,
    finetune_method: str,
    datasets: DatasetDict,
    output_dir_base: str,
    num_epochs: int
):
    """
    æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒã€è¯„ä¼°å’Œä¿å­˜æµç¨‹ã€‚
    """
    # æ¸…ç† backbone åç§°ï¼Œä½¿å…¶é€‚ç”¨äºæ–‡ä»¶è·¯å¾„
    backbone_sanitized = backbone.replace("/", "_")
    run_name = f"{backbone_sanitized}_{finetune_method}"
    output_dir = Path(output_dir_base) / backbone_sanitized / finetune_method
    
    logging.info("="*80)
    logging.info(f"ğŸš€ å¼€å§‹æ–°ä¸€è½®å®éªŒ: {run_name}")
    logging.info(f"   - æ¨¡å‹è¾“å‡ºè·¯å¾„: {output_dir}")
    logging.info("="*80)

    # åˆå§‹åŒ– WandB
    try:
        wandb.init(
            project=WANDB_PROJECT_NAME,
            name=run_name,
            reinit=True,
            config={
                "backbone": backbone,
                "finetune_method": finetune_method,
                "num_epochs": num_epochs,
            }
        )
    except Exception as e:
        logging.warning(f"WandB åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ç¦ç”¨æ—¥å¿—è®°å½•: {e}")
        os.environ["WANDB_DISABLED"] = "true"


    # åŠ è½½æ¨¡å‹å’Œ Tokenizer
    model, tokenizer = get_model_and_tokenizer(backbone, finetune_method)

    # æ•°æ®é¢„å¤„ç†
    logging.info("â‘¢ æ•°æ®é¢„å¤„ç†...")
    preprocess_function = create_preprocess_function(tokenizer)
    tokenized_datasets = datasets.map(preprocess_function, batched=True)

    # è®¾ç½®è®­ç»ƒå‚æ•°
    logging.info("â‘£ è®¾ç½®è®­ç»ƒå‚æ•°...")
    training_args = TrainingArguments(
        output_dir=str(output_dir),
        num_train_epochs=num_epochs,
        per_device_train_batch_size=256,
        per_device_eval_batch_size=256,
        warmup_steps=50,
        weight_decay=0.01,
        logging_dir=f'{output_dir}/logs',
        logging_steps=10,
        evaluation_strategy="epoch",  # æ¯ä¸ª epoch ç»“æŸåè¿›è¡Œè¯„ä¼°
        save_strategy="epoch",        # æ¯ä¸ª epoch ç»“æŸåä¿å­˜æ¨¡å‹
        load_best_model_at_end=True,  # è®­ç»ƒç»“æŸååŠ è½½æœ€ä½³æ¨¡å‹
        metric_for_best_model="accuracy", # ä½¿ç”¨å‡†ç¡®ç‡ä½œä¸ºè¯„ä¼°æŒ‡æ ‡
        greater_is_better=True,
        report_to="wandb" if os.environ.get("WANDB_DISABLED") != "true" else "none",
        fp16=torch.cuda.is_available(), # å¦‚æœæœ‰ GPUï¼Œä½¿ç”¨åŠç²¾åº¦è®­ç»ƒ
    )

    # åˆå§‹åŒ– Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        compute_metrics=compute_metrics,
    )

    # å¼€å§‹è®­ç»ƒ
    logging.info("â‘¤ å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    logging.info("è®­ç»ƒå®Œæˆã€‚")

    # åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°
    logging.info("â‘¥ åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
    test_results = trainer.predict(tokenized_datasets["test"])
    logging.info(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_results.metrics['test_accuracy']:.4f}")
    
    # å°†æµ‹è¯•ç»“æœä¸ŠæŠ¥åˆ° WandB
    if os.environ.get("WANDB_DISABLED") != "true":
        wandb.log({"test_accuracy": test_results.metrics['test_accuracy']})

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œ Tokenizer
    logging.info(f"â‘¦ ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° {output_dir}...")
    trainer.save_model(str(output_dir))
    tokenizer.save_pretrained(str(output_dir))
    
    # å¦‚æœæ˜¯ PEFT æ¨¡å‹ï¼Œä¹Ÿä¿å­˜ PEFT é…ç½®
    if isinstance(model, PeftModel):
        model.peft_config[model.active_adapter].save_pretrained(str(output_dir))

    logging.info(f"å®éªŒ {run_name} å®Œæˆã€‚")
    if os.environ.get("WANDB_DISABLED") != "true":
        wandb.finish()


def main():
    """
    ä¸»å‡½æ•°ï¼Œè§£æå‘½ä»¤è¡Œå‚æ•°å¹¶å¯åŠ¨å®éªŒæµç¨‹ã€‚
    """
    parser = argparse.ArgumentParser(description="å¤šæ¨¡å‹ã€å¤šç­–ç•¥å¾®è°ƒå®éªŒæ¡†æ¶")
    parser.add_argument(
        "--data_path",
        type=str,
        required=True,
        help="åŒ…å« 'instruction' å’Œ 'label' çš„ JSON æ•°æ®æ–‡ä»¶è·¯å¾„ã€‚"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./finetune_results",
        help="ä¿å­˜æ¨¡å‹å’Œæ—¥å¿—çš„æ ¹ç›®å½•ã€‚"
    )
    parser.add_argument(
        "--num_epochs",
        type=int,
        default=1,
        help="è®­ç»ƒçš„è½®æ¬¡ï¼ˆEpochsï¼‰ã€‚"
    )
    parser.add_argument(
        "--backbones",
        nargs='+',
        default=SUPPORTED_BACKBONES,
        help=f"è¦æµ‹è¯•çš„é¢„è®­ç»ƒæ¨¡å‹åˆ—è¡¨ã€‚æ”¯æŒ: {', '.join(SUPPORTED_BACKBONES)}"
    )
    parser.add_argument(
        "--finetune_methods",
        nargs='+',
        default=SUPPORTED_FINETUNE_METHODS,
        help=f"è¦æµ‹è¯•çš„å¾®è°ƒç­–ç•¥åˆ—è¡¨ã€‚æ”¯æŒ: {', '.join(SUPPORTED_FINETUNE_METHODS)}"
    )
    args = parser.parse_args()

    # 1. åŠ è½½å’Œåˆ’åˆ†æ•°æ®ï¼ˆä¸€æ¬¡æ€§å®Œæˆï¼‰
    datasets = load_and_split_data(args.data_path)

    # 2. éå†æ‰€æœ‰æ¨¡å‹å’Œå¾®è°ƒç­–ç•¥çš„ç»„åˆ
    for backbone in args.backbones:
        if backbone not in SUPPORTED_BACKBONES:
            logging.warning(f"è·³è¿‡ä¸æ”¯æŒçš„æ¨¡å‹: {backbone}")
            continue
        for method in args.finetune_methods:
            if method not in SUPPORTED_FINETUNE_METHODS:
                logging.warning(f"è·³è¿‡ä¸æ”¯æŒçš„å¾®è°ƒæ–¹æ³•: {method}")
                continue
            
            # --- æ–°å¢çš„æ£€æŸ¥é€»è¾‘ ---
            # æå‰æ„å»ºè¾“å‡ºç›®å½•è·¯å¾„ä»¥è¿›è¡Œæ£€æŸ¥
            backbone_sanitized = backbone.replace("/", "_")
            output_dir = Path(args.output_dir) / backbone_sanitized / method
            
            # æ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦å·²å­˜åœ¨ä¸”ä¸ä¸ºç©ºï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è·³è¿‡
            if output_dir.exists() and any(output_dir.iterdir()):
                logging.info(f"âœ… ç»“æœç›®å½• {output_dir} å·²å­˜åœ¨ï¼Œè·³è¿‡å®éªŒ: {backbone_sanitized}_{method}")
                continue
            # --- ç»“æŸæ–°å¢é€»è¾‘ ---

            # ä¸ºæ¯ä¸ªç»„åˆè¿è¡Œè®­ç»ƒæµç¨‹
            try:
                run_training_pipeline(
                    backbone=backbone,
                    finetune_method=method,
                    datasets=datasets,
                    output_dir_base=args.output_dir,
                    num_epochs=args.num_epochs
                )
            except Exception as e:
                logging.error(f"å®éªŒå¤±è´¥: backbone={backbone}, method={method}")
                logging.error(f"é”™è¯¯è¯¦æƒ…: {e}", exc_info=True)
                # å³ä½¿ä¸€ä¸ªå®éªŒå¤±è´¥ï¼Œä¹Ÿç»§ç»­ä¸‹ä¸€ä¸ª
                if os.environ.get("WANDB_DISABLED") != "true" and wandb.run is not None:
                    wandb.finish(exit_code=1) # æ ‡è®°æ­¤ wandb run å¤±è´¥
                continue
    
    logging.info("æ‰€æœ‰å®éªŒå·²å®Œæˆï¼")


if __name__ == "__main__":
    main()
